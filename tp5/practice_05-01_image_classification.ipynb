{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPITA 2023 MLRF practice_05-01_image_classification v2023-06-11_215901 by Joseph CHAZALON\n",
    "\n",
    "<div style=\"overflow: auto; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"Creative Commons License\" src='img/CC-BY-4.0.png' style='float: left; margin-right: 20px'>\n",
    "    \n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 05 part 01: Image classification\n",
    "\n",
    "## 0. Introduction\n",
    "\n",
    "We will demonstrate how to build on the BoVW technique you implemented for the last session in order to recognize the category an image belongs to.\n",
    "\n",
    "It consists in building a simple classifier using the BoVW descriptors which can be computed with the same method as the practice session 4.\n",
    "However, you way want to seize this opportunity to improve the method you implemented, and maybe use a more powerful feature encoding technique, like the VLAD or even the Fisher Vector… Or extract better descriptors… Just make sure you **first complete a basic pipeline**, and select the **best possible classifier** for your encoding.\n",
    "\n",
    "\n",
    "This session is inspired by this cool 2018 meme about animals which look like food:\n",
    "![Cranberry muffins and chihuahuas](img/practice_05/meme_full.jpg)\n",
    "\n",
    "\n",
    "In this session, you will learn how to produce results like the ones displayed below (first rows: chihuahuas, second rows: cranberry muffins, color and title indicates whether the classifier produced the right result).\n",
    "![Sample output](img/practice_05/results.png)\n",
    "\n",
    "\n",
    "\n",
    "We will proceed in 9 steps:\n",
    "\n",
    "1. Load resources\n",
    "2. Train a BoVW model\n",
    "3. Split the dataset into training and validation sets\n",
    "4. Compute the BoVW descriptor for each image\n",
    "5. Prepare training structures\n",
    "6. Train a classifier and evaluate its performance\n",
    "7. Display some results\n",
    "8. Test on meme images\n",
    "9. Compute the results on the test set and export them"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Resources\n",
    "The resources for this session are stored on the network.\n",
    "\n",
    "Here is a description of the files we provide in the student's archive:\n",
    "- `meme_jpg/`: contains original meme images, for fun;\n",
    "- `meme_siftgeo/`: contains SIFT descriptors of original meme images, for fun;\n",
    "- `test_set_sift/`: contains the SIFT descriptors of the 400 test set images;\n",
    "- `train_val_set_jpg/`: contains 2694 training and validation images for both classes:\n",
    "   1416 for the \"chihuahua\" (`chi`) class, and 1278 for the \"muffin\" (`muf`) class;\n",
    "- `train_val_set_siftgeo/`: contains the SIFT descriptors of the training and validation images\n",
    "   for both classes;\n",
    "- `sample_results.json`: an example of result file (with random results);\n",
    "- `README.md`: even more details about the dataset.\n",
    "\n",
    "**IMPORTANT:** the training set is **IMBALANCED** and you should take this into account before training\n",
    "any classifier. The test set **is balanced**.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2 Ground truth format\n",
    "The ground truth is implicit in this session: the files are stored in directories indicating the category they belong to. This is a very common practice among the various public dataset we can find."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3  Local descriptors\n",
    "We provide precomputed SIFT descriptors to save you time.\n",
    "We also provide a commodity function to help you loading them (see below), just like for the previous session."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivate buggy jupyter completion\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sklearn\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Make sure the resources location below is correct.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the resources location\n",
    "PATH_TO_RESOURCES = \"/afs/cri.epita.fr/resources/teach/bigdata/mlrf21/muf_vs_chi/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to read descriptors\n",
    "We provide you with the function `siftgeo_read_desc(path)` which allows to get the array of descriptors stored in a `.siftgeo` file.\n",
    "\n",
    "#### About the SIFT descriptors used (`.siftgeo` files)\n",
    "Descriptors are stored in raw together with the region information provided by the software of Krystian Mikolajczyk.\n",
    "There is no header (use the file length to find the number of descriptors).\n",
    "A descriptor takes 168 bytes (floats and ints take 4 bytes, and are stored in little endian):\n",
    "```\n",
    "field      field type   description\n",
    "x          float        horizontal position of the interest point\n",
    "y          float        vertical position of the interest point\n",
    "scale      float        scale of the interest region\n",
    "angle      float        angle of the interest region\n",
    "mi11       float        affine matrix component\n",
    "mi12       float        affine matrix component\n",
    "mi21       float        affine matrix component\n",
    "mi22       float        affine matrix component\n",
    "cornerness float        saliency of the interest point\n",
    "desdim     int          dimension of the descriptors\n",
    "component  byte*desdim  the descriptor vector (dd components)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIFTGEO_DTYPE = np.dtype([\n",
    "    (\"x\", \"<f4\"),\n",
    "    (\"y\", \"<f4\"),\n",
    "    (\"scale\", \"<f4\"),\n",
    "    (\"angle\", \"<f4\"),\n",
    "    (\"mi11\", \"<f4\"),\n",
    "    (\"mi12\", \"<f4\"),\n",
    "    (\"mi21\", \"<f4\"),\n",
    "    (\"mi22\", \"<f4\"),\n",
    "    (\"cornerness\", \"<f4\"),\n",
    "    (\"desdim\", \"<i4\"),\n",
    "    (\"component\", \"<u1\", 128)\n",
    "])\n",
    "\n",
    "def siftgeo_read_full(path):\n",
    "    return np.fromfile(path, dtype=SIFTGEO_DTYPE)\n",
    "\n",
    "def siftgeo_read_desc(path):\n",
    "    if not path.endswith(\".siftgeo\"):\n",
    "        raise ValueError(\"'path' parameter should end with '.siftgeo'.\")\n",
    "    desc = siftgeo_read_full(path)[\"component\"]\n",
    "    if desc.size == 0: \n",
    "        desc = np.zeros((0, 128), dtype = 'uint8')\n",
    "    return desc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare some list of files, for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using WINDOWS? (if not just remove this cell)\n",
    "# ==============\n",
    "# You may want to replace the commands below with something like:\n",
    "# ALL_IMGS_CHI = !bash -c \"ls $PATH_TO_RESOURCES/train_val_set_jpg/chi/*.jpg | sort\"\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of paths to training CHI images (only used for display)\n",
    "ALL_IMGS_CHI = !ls $PATH_TO_RESOURCES/train_val_set_jpg/chi/*.jpg | sort\n",
    "# list of paths to the training CHI descriptors for each image (same order)\n",
    "ALL_SIFT_CHI = !ls $PATH_TO_RESOURCES/train_val_set_siftgeo/chi/*.siftgeo | sort\n",
    "# list of paths to training MUF images (only used for display)\n",
    "ALL_IMGS_MUF = !ls $PATH_TO_RESOURCES/train_val_set_jpg/muf/*.jpg | sort\n",
    "# list of paths to the training MUF descriptors for each image (same order)\n",
    "ALL_SIFT_MUF = !ls $PATH_TO_RESOURCES/train_val_set_siftgeo/muf/*.siftgeo | sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SIFT_CHI[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SIFT_MUF[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train a BoVW model\n",
    "We want to assign a single vector to each image we will encounter.\n",
    "\n",
    "We first need to build a BoVW model, exactly like during the previous session."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Select training descriptors\n",
    "We need **some** descriptors from **every** classes!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Select and load descriptors for 100 images of each category.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO select files to load\n",
    "# sample_descr_chi = ???\n",
    "# sample_descr_chi[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO select files to load\n",
    "# sample_descr_muf = ???\n",
    "# sample_descr_muf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO load descriptors as a list of np.array (each np.array contains the descriptors for an image)\n",
    "train_desc = []\n",
    "# for ...  # FIXME\n",
    "len(train_desc)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of arrays of SIFT descriptors: for each image we have an array of shape `(number of descriptors, SIFT descriptor size)`.\n",
    "\n",
    "We need to stack those descriptors to produce one big training array of shape `(total number of training descriptors, SIFT descriptor size)`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Stack your descriptors to prepare your training set for the codebook generation.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# train_desc = ???\n",
    "# train_desc.shape, train_desc.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the following result from the previous cell:\n",
    "`((83720, 128), dtype('uint8'))`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert this array to float32 elements to avoid normalization issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = train_desc.astype(np.float32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Learn normalization parameters and train a k-Means\n",
    "We now need to perform some preprocessing on our data: centering and dimension reduction, before training our k-Means."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the mean of your sample and center your data.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# compute mean and center descriptors\n",
    "# train_mean = ???\n",
    "# train_desc = ???"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Learn a PCA transform to reduce the number of dimensions used.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PCA matrix and keep only 64 dimensions\n",
    "train_cov = np.dot(train_desc.T, train_desc)\n",
    "eigvals, eigvecs = np.linalg.eig(train_cov)\n",
    "perm = eigvals.argsort()                   # sort by increasing eigenvalue\n",
    "pca_transform = eigvecs[:, perm[64:128]]   # eigenvectors for the 64 last eigenvalues\n",
    "pca_transform.shape, pca_transform.dtype"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Apply the PCA transform to your data.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# transform sample with PCA (note that numpy imposes line-vectors,\n",
    "# so we right-multiply the vectors)\n",
    "# train_desc = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of your new descriptors: we expect a shape like (BIGNUMBER, 64)\n",
    "train_desc.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Train a k-Means with 512 dimensions to compute our codebook.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# fit k-means\n",
    "# kmeans = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the shape of the cluster's centers\n",
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know have a codebook, and some learned parameters for preprocessing our data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Split the dataset into training and validation sets\n",
    "Before encoding our features, to avoid computing large arrays and splitting them afterhand (hence consuming much memory because of Python computation model), we will first separate the list of files into appropriate sublists for training and validating our model.\n",
    "\n",
    "**But even before** we need to **make sure our dataset is balanced**!\n",
    "\n",
    "Why? Well keep in mind that discriminant classifiers are very sensitive to class imbalance: if the density of elements is not the same for both classes, the decision boundary will be pushed toward the class with lower density because there will be less classification errors here!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Transform the lists of files we previously defined to make sure our dataset is balanced.**\n",
    "</div>\n",
    "\n",
    "*Tip:* Here are the lists we previously defined:\n",
    "- `ALL_IMGS_CHI`\n",
    "- `ALL_SIFT_CHI`\n",
    "- `ALL_IMGS_MUF`\n",
    "- `ALL_SIFT_MUF`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ALL_IMGS_CHI), len(ALL_IMGS_MUF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# transform the lists\n",
    "# ALL_IMGS_CHI = ???\n",
    "# ALL_SIFT_CHI = ???\n",
    "# ALL_IMGS_MUF = ???\n",
    "# ALL_SIFT_MUF = ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ALL_IMGS_CHI), len(ALL_IMGS_MUF)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Using Scikit learn function `sklearn.model_selection.train_test_split(...)`, separate the list of files we previously computed into two subsets each: a training set and a validation set.**\n",
    "</div>\n",
    "\n",
    "*Tip:* Here are the lists we previously defined:\n",
    "- `ALL_IMGS_CHI`\n",
    "- `ALL_SIFT_CHI`\n",
    "- `ALL_IMGS_MUF`\n",
    "- `ALL_SIFT_MUF`\n",
    "\n",
    "We also want to split the list of images because it will be useful for visualizing the results.\n",
    "\n",
    "You also may want to **specify the random state** to be able to reproduce your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# train / validation split\n",
    "# (train_imgs_chi, val_imgs_chi,\n",
    "#  train_sift_chi, val_sift_chi,\n",
    "#  train_imgs_muf, val_imgs_muf,\n",
    "#  train_sift_muf, val_sift_muf) = sklearn.model_selection.train_test_split(\n",
    "#     ???\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_sift_chi), len(val_sift_chi), len(train_sift_muf), len(val_sift_muf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**According to you, why do we call our sets \"training\" and \"validation\", and not \"training\" and \"testing\"?**\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute the BoVW descriptor for each image\n",
    "We can now encode the local SIFT descriptors of each image using the elements we learned.\n",
    "\n",
    "Like in the previous session, we will compte a single descriptor of dimension `512` (or something else if you changed the size of the coodebook).\n",
    "\n",
    "We will however add a couple of improvements over the version used during the last session: \n",
    "1. we will project our descriptor to another embedding space using the square root, to simulate an Hellinger kernel;\n",
    "2. we will perform a L2 normalization of the final descriptor to facilitate linear classification."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import an appropriate normalizer for L2, to avoid little programming mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_normalizer = sklearn.preprocessing.Normalizer(norm='l2', copy=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Complete the function below which takes a list of `.siftgeo` files, a training mean, a PCA transform, and a k-Means object and compute the arrays of BoVW descriptors, one for each `.siftgeo` file.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "def compute_descriptors(siftgeo_filelist, train_mean, pca_transform, kmeans):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    siftgeo_filelist: list of len `num_files`\n",
    "        List of paths to SIFTGEO files\n",
    "    train_mean: np.array of shape (descr_size, )\n",
    "        Mean value of the descriptor space (previously learned)\n",
    "    pca_transform: np.array of shape (descr_size, final_dim)\n",
    "        Matrix of the projection transform from the original descriptor space to the final space\n",
    "    kmeans: sklearn.MiniBatchKMeans witk `n_clusters` centroids\n",
    "        KMeans object with a `predict` method\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    image_descriptors: np.array of size (num_files, n_clusters)\n",
    "        Global descriptor computed from each original SIFTGEO file (set of local descriptors).\n",
    "    \"\"\"\n",
    "    image_descriptors = np.zeros((len(siftgeo_filelist), kmeans.n_clusters), dtype=np.float32)\n",
    "    \n",
    "    for ii, descrfile in enumerate(siftgeo_filelist):\n",
    "        shortname = descrfile.split('/')[-2:]\n",
    "        print(\"Indexing %s\" % (shortname,))\n",
    "        # read the descriptors\n",
    "        desc = siftgeo_read_desc(descrfile)\n",
    "        if desc.shape[0] == 0:\n",
    "            # let the descriptor be 0 for all values\n",
    "            # note that this is bad and the element should be dropped from the index\n",
    "            print(\"WARNING: zero descriptor for %s\" % (shortname,))\n",
    "            continue\n",
    "        \n",
    "        # convert to float\n",
    "        desc = desc.astype(np.float32)\n",
    "        \n",
    "        # center and apply PCA transform\n",
    "#         desc = ???  # FIXME\n",
    "        \n",
    "        # get cluster ids\n",
    "#         clabels = ???  # FIXME\n",
    "        # compute histogram\n",
    "#         descr_hist = ???  # FIXME\n",
    "        \n",
    "        # l1 norm\n",
    "#         descr_hist = ???  # FIXME\n",
    "        \n",
    "        # take the sqrt (Hellinger kernel)\n",
    "#         descr_hist = ???  # FIXME\n",
    "        \n",
    "        # l2 norm (not necessary here --- already L2 normalized, TODO check the math)\n",
    "#         descr_hist = ???  # FIXME\n",
    "        \n",
    "        # update the index\n",
    "        image_descriptors[ii] = descr_hist\n",
    "    print(\"Indexing complete.\")\n",
    "    return image_descriptors"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can now computes the descriptors for each subset.**\n",
    "\n",
    "Let us just run the previously-defined function in batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_chi, x_val_chi, x_train_muf, x_val_muf = [\n",
    "    compute_descriptors(filelist, train_mean, pca_transform, kmeans) \n",
    "    for filelist in (train_sift_chi, val_sift_chi,\n",
    "                     train_sift_muf, val_sift_muf)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare training structures\n",
    "\n",
    "To train our classifier, we need to prepare two structures:\n",
    "- a `X` array containing of shape `(n_samples, n_features)`;\n",
    "- a `y` array containing the target values to predict."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of Data in Scikit-learn\n",
    "\n",
    "Machine learning is about creating models from data: for that reason, we'll start by\n",
    "discussing how data can be represented in order to be understood by the computer.  Along\n",
    "with this, we'll build on our matplotlib examples from the previous section and show some\n",
    "examples of how to visualize data.\n",
    "\n",
    "Most machine learning algorithms implemented in scikit-learn expect data to be stored in a\n",
    "**two-dimensional array or matrix**.  The arrays can be\n",
    "either ``numpy`` arrays, or in some cases ``scipy.sparse`` matrices.\n",
    "The size of the array is expected to be `[n_samples, n_features]`\n",
    "\n",
    "- **n_samples:**   The number of samples: each sample is an item to process (e.g. classify).\n",
    "  A sample can be a document, a picture, a sound, a video, an astronomical object,\n",
    "  a row in database or CSV file,\n",
    "  or whatever you can describe with a fixed set of quantitative traits.\n",
    "- **n_features:**  The number of features or distinct traits that can be used to describe each\n",
    "  item in a quantitative manner.  Features are generally real-valued, but may be boolean or\n",
    "  discrete-valued in some cases.\n",
    "\n",
    "The number of features must be fixed in advance. However it can be very high dimensional\n",
    "(e.g. millions of features) with most of them being zeros for a given sample. This is a case\n",
    "where `scipy.sparse` matrices can be useful, in that they are\n",
    "much more memory-efficient than numpy arrays.\n",
    "\n",
    "![Data Layout](img/practice_05/data-layout.png)\n",
    "\n",
    "(From the [Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Create the \"X\" training vector using the previously generated descriptors.**\n",
    "</div>\n",
    "\n",
    "*Tip:* Its is just about stacking `x_train_chi` and `x_train_muf`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO prepare X training data\n",
    "# x_train = ???\n",
    "# x_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Create the \"y\" training vector. It is slightly trickier: we will create a vector of shape `(n_images, )` with `0`s for elements belonging to the first class (`chi`), and `1`s for elements belonging to the second class (`muf`).**\n",
    "</div>\n",
    "\n",
    "*Tip:* Our dataset is balanced and we created our `x_train` vector in such way that the first half of the samples belong to the first category, and the second half to the other one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create y, the target vector for training data, and fill its values appropriately\n",
    "# y_train = ???\n",
    "# y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: count the number of elements in the second class\n",
    "np.sum(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Conversely, create the validation source and target vectors which will allow us to measure the performance of our classifier.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# x_val = ???\n",
    "# x_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# y_val = ???\n",
    "# y_val.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train a classifier and evaluate its performance\n",
    "We are now ready to enjoy the beauty of Scikit-learn: we will create a simple linear SVM with default values, train it and evaluate its performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Create a simple linear SVM classifier.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create your classifier\n",
    "# from sklearn.??? import ???\n",
    "# clf = ???"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Train your classifier using `x_train` and `y_train`.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# clf.????"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Now evaluate the performance of your classifier using one of its builtin methods. Which \"X\" and \"y\" are you going to use? What is the measure we obtain?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate the performance of your classifier on the VALIDATION set\n",
    "# clf.???"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a basic BoVW implementation, we obtained around 70% accuracy on a validation set.\n",
    "\n",
    "Keep in mind that the dataset is noisy and that the accuracy on the test set may be quite different."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compare the score you obtained on the validation set with the one on the training set. What does this reveal?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO evaluate the performance of your classifier on the TRAINING set\n",
    "# clf.???"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**What would be the performance of a random classifier? Are we doing better than chance?**\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO your answer here"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display some results\n",
    "We provide you with some code to display some results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.image import imread\n",
    "\n",
    "nelem = 20   # number of elements to show\n",
    "\n",
    "def show_image(imgpath, frame_color, gray=True):\n",
    "    im = imread(imgpath)\n",
    "    if gray:\n",
    "        plt.imshow(im, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(im)\n",
    "    h, w = im.shape[:2]\n",
    "    plt.plot([0, 0, w, w, 0], [0, h, h, 0, 0], frame_color, linewidth = 2)\n",
    "    plt.axis('off')\n",
    "\n",
    "# reduce the margins\n",
    "plt.subplots_adjust(wspace = 0, hspace = 0,\n",
    "                    top = 0.99, bottom = 0.01, left = 0.01, right = 0.99)\n",
    "\n",
    "plt.figure(figsize=(4,40))\n",
    "no = 1  # index current of subfigure\n",
    "for ii in range(nelem):\n",
    "    plt.subplot(nelem, 2, 2*ii+1)\n",
    "    val_img_i = val_imgs_chi[ii]\n",
    "    x_val_i = x_val_chi[ii]\n",
    "    y_pred_i = clf.predict(x_val_i.reshape(1,-1))\n",
    "    expected = 0; classname = \"CHI\"\n",
    "    show_image(val_img_i, 'g' if y_pred_i == expected else 'r')\n",
    "    plt.title(classname + \" \" + (\"OK\" if y_pred_i == expected else \"ERR\"))\n",
    "    \n",
    "    plt.subplot(nelem, 2, 2*ii+2)\n",
    "    val_img_i = val_imgs_muf[ii]\n",
    "    x_val_i = x_val_muf[ii]\n",
    "    y_pred_i = clf.predict(x_val_i.reshape(1,-1))\n",
    "    expected = 1; classname = \"MUF\"\n",
    "    show_image(val_img_i, 'g' if y_pred_i == expected else 'r')\n",
    "    plt.title(classname + \" \" + (\"OK\" if y_pred_i == expected else \"ERR\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Test on meme images\n",
    "Time for fun: let's test our classifier on the meme images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_test_sifts = !ls $PATH_TO_RESOURCES/meme_siftgeo/*.siftgeo | sort\n",
    "meme_test_sifts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meme_test_imgs = !ls $PATH_TO_RESOURCES/meme_jpg/*.j*g | sort\n",
    "meme_test_imgs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the descriptors for each image.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# x_meme_test = ???\n",
    "# x_meme_test.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the target values for the `x_meme_test` vector you created.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# y_true_meme_test = ???\n",
    "# y_true_meme_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the score of our classifier on this little sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(x_meme_test, y_true_meme_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now display the results for the meme images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the predicted values for all meme images.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# y_pred_meme_test = clf.???\n",
    "# y_pred_meme_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a little code to render the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,4))\n",
    "for ii, (img, y_pred, y_true) in enumerate(zip(meme_test_imgs, y_pred_meme_test, y_true_meme_test)):\n",
    "    plt.subplot(2, 8, ii+1)\n",
    "    correct = y_pred == y_true\n",
    "    show_image(img, 'g' if correct else 'r')\n",
    "    plt.title((\"CHI\" if y_true == 0 else \"MUF\") + \" \" + (\"OK\" if correct else \"ERR\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Can you tell anything about the performance of your system based on these results?**\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO So, what can we say?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Compute the results on the test set and export them\n",
    "You are now ready to process the test data and submit them for grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sift_files = !ls $PATH_TO_RESOURCES/test_set_sift/*.siftgeo | sort\n",
    "test_sift_files[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the BoVW descriptors for each `.siftgeo` file, compute the predicted class, and build the map of results `{test_id: class_name}`.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute the descriptors for the test set\n",
    "# x_test = compute_descriptors(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute the predicted classes for the test set\n",
    "# y_test_pred = ???"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below create the structure of the result file automatically, given the source files of the test set and the predictions your system made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is an utility fonction to extract the appropriate part from the path to generate the result id\n",
    "def path_to_id(path):\n",
    "    return path[-(9+8):-8]\n",
    "\n",
    "assert(path_to_id(test_sift_files[0]) == 'test_0000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {path_to_id(f):(\"chi\" if y == 0 else \"muf\") for f,y in zip(test_sift_files, y_test_pred)}\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results.json\", 'w') as outfile:\n",
    "    json.dump(results, outfile, indent=1)\n",
    "!head results.json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final file should have a content like:\n",
    "```json\n",
    "{\n",
    " 'test_0000': 'chi',\n",
    " 'test_0001': 'muf',\n",
    " 'test_0002': 'muf',\n",
    " ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job done!\n",
    "You completed this session, congratulations!\n",
    "\n",
    "**Do not forget to submit your `results.json` and `*.ipynb` files!**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 331,
   "position": {
    "height": "353px",
    "left": "838px",
    "right": "20px",
    "top": "207px",
    "width": "542px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
