{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPITA 2022 MLRF practice_03-01_ORB_AR v2023-05-30_183713 by Joseph CHAZALON\n",
    "\n",
    "<div style=\"overflow: auto; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"Creative Commons License\" src='img/CC-BY-4.0.png' style='float: left; margin-right: 20px'>\n",
    "    \n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 03 part 01: Augmented Reality using ORB matching\n",
    "\n",
    "We will demonstrate a simple technique, light enough to be run on an old smartphone, which detects an instance of a know document in a video frame, and overlays some dynamic content over this document in the frame.\n",
    "\n",
    "We will use an excerpt of a dataset we created for a funny little app a few years ago, which allows children to point at a songbook page and play the associated song using a tablet. This is illustrated below.\n",
    " \n",
    "\n",
    "![AR example](img/practice_03/AR2.jpg)\n",
    "\n",
    "This is much like marker-based Augmented Reality (AR), where the marker is a complex image.\n",
    "\n",
    "**This approach requires to prepare of document model prior to matching documents within frames.**\n",
    "\n",
    "We will proceed in 5 steps:\n",
    "\n",
    "1. detect keypoints from the model and display them;\n",
    "2. compute the descriptors for the model image;\n",
    "3. detect the keypoints for a frame image, compute the descriptors and display them;\n",
    "4. create matchers and index descriptors;\n",
    "5. estimate the homography from the model to the frame;\n",
    "5. project a modified image over the document in the frame.\n",
    "\n",
    "The resources for this session are packaged directly within this notebook's archive: you can access them under the `resources/` folder:\n",
    "- `model.png`: the model image we will use;\n",
    "- `frame_0010.jpeg`: a frame image extracted from a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import module, load resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deactivate buggy jupyter completion\n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tested this lab session using OpenCV 4.0.0. Beware of API breaks with version 5!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_RESOURCES = \"./resources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img = cv2.imread(os.path.join(PATH_TO_RESOURCES, \"model.png\"))\n",
    "model_img.shape, model_img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remain sane\n",
    "def bgr2rgb(img):\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is our model image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bgr2rgb(model_img), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert it to grayscale to extract ORB keypoints from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img_gray = cv2.cvtColor(model_img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(model_img_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the frame we will process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_img = cv2.imread(os.path.join(PATH_TO_RESOURCES, \"frame_0010.jpeg\"))\n",
    "frame_img.shape, model_img.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(bgr2rgb(frame_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to convert it to grayscale, for the same reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_img_gray = cv2.cvtColor(frame_img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_img_gray, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detect and draw keypoints (model image)\n",
    "First, we will detect and display some keypoints using the ORB method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Complete the creation of the ORB object below, setting parameters appropriately.**\n",
    "\n",
    "*Tips:*\n",
    "- ORB keypoint detection and description is performed with the same `ORB` object.\n",
    "- Create this `ORB` object using `cv2.ORB.create(...)`.\n",
    "- You need to select appropriate parameters. The default parameters may not give the best possible results.\n",
    "- In particular, we need:\n",
    "  - a few thousands features;\n",
    "  - several levels (as the frame object may appear several times smaller than in the original model);\n",
    "  - to select the Harris score to stabilize the results (`scoreType=cv2.ORB_HARRIS_SCORE`).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me!\n",
    "cv2.ORB.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO create the ORB detector and descriptor\n",
    "# orb = cv2.ORB.create(...) # FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Now you can detect keypoints from the model image.**\n",
    "\n",
    "*Tips:*\n",
    "- Use the `orb.detect()` method.\n",
    "- Use the graylevel image.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO detect keypoints\n",
    "# model_kpts = # FIXME\n",
    "# len(model_kpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Display the keypoints using the function we provide below.**\n",
    "</div>\n",
    "\n",
    "Expected result:\n",
    "![](img/practice_03/out1_kpts.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the function from OpenCV's python wrapper is buggy\n",
    "def draw_keypoints(color_image, keypoints, color=(0,255,0)):\n",
    "    '''\n",
    "    Display keypoints in some color over an image.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    color_image: ndarray, shape=(rows, cols, 3 channels)\n",
    "        color image in BGR order\n",
    "\n",
    "    keypoints: list of cv2.KeyPoint\n",
    "        keypoints detected in the image\n",
    "\n",
    "    color: tuple of uint8 (optional)\n",
    "        color of the keypoints to drawn, in BGR order\n",
    "    '''\n",
    "    if color_image.ndim != 3:\n",
    "        raise ValueError(\n",
    "            \"draw_keypoints: parameter `color_image` must be a... (wait for it) color image!\")\n",
    "    draw = color_image.copy()\n",
    "    for k in keypoints:\n",
    "        angle = k.angle\n",
    "        class_id = k.class_id\n",
    "        convert = k.convert\n",
    "        octave = k.octave\n",
    "        overlap = k.overlap\n",
    "        pt_x, pt_y = k.pt\n",
    "        pt_int = int(pt_x), int(pt_y)\n",
    "        response = k.response\n",
    "        size = k.size\n",
    "        cv2.circle(draw, pt_int, int(size), color)\n",
    "        pt2 = int(pt_x + np.sin(angle)*size), int(pt_y + np.cos(angle)*size)\n",
    "        cv2.line(draw, pt_int, pt2, color, thickness=2)\n",
    "    plt.imshow(bgr2rgb(draw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO draw the keypoints detected in the model image\n",
    "# draw_keypoints(...) # FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute descriptors (model image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the descriptors for each of the keypoints we previously detected.**\n",
    "\n",
    "*Tips:*\n",
    "- Use the `ORB.compute() method.`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute the descriptors\n",
    "# model_kpts, model_desc = ... # FIXME\n",
    "# len(model_kpts), model_desc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**What is the size (in bytes) of an ORB descriptor?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO you answer here**\n",
    "\n",
    "Storing an ORB descriptor takes ... bytes (without indexing overhead)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detect and compute keypoints from the frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Using the `ORB.detectAndCompute()` method, preform keypoint detection and description in a single step.**\n",
    "\n",
    "*Tips:*\n",
    "- This function requires a mask but we do not need it. Set `mask=None`.\n",
    "</div>\n",
    "\n",
    "\n",
    "Expected result of `draw_keypoints()`:\n",
    "![](img/practice_03/out2_kpts_frame.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO detect keypoints and compute descriptors for the frame\n",
    "# frame_kpts, frame_descr = # FIXME\n",
    "# len(frame_kpts), frame_descr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me!\n",
    "draw_keypoints(frame_img, frame_kpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**What are the regions where keypoints are detected?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO you answer here**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a matcher and index model descriptors\n",
    "A matcher object is used to compare two sets of descriptors.\n",
    "\n",
    "The relevant OpenCV documentation is available at the [DescriptorMatcher documentation page](https://docs.opencv.org/master/db/d39/classcv_1_1DescriptorMatcher.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "There are two matchers available in OpenCV: \n",
    "- the brute force matcher, which performs a $m \\times n$ match of each of the $m$ descriptors in the first set against each of the $n$ descriptors in the second set;\n",
    "- the FLANN-based matcher, which performs fast approximate nearest neighbor search using an indexing structure.\n",
    "\n",
    "In both cases, we need to specifiy the **distance** the matcher will use to compare descriptors.\n",
    "There are several built-in norms:\n",
    "- cv2.NORM_INF: $\\|X-Y\\|_{L_{\\infty}} = \\max _i | X_i - Y_i|$ where $V_i$ is the $i$-th component of vector $V$.\n",
    "- cv2.NORM_L1: $\\| X-Y \\| _{L_1} = \\sum _i | X_i - Y_i|$\n",
    "- cv2.NORM_L2: $\\| X-Y \\| _{L_2} = \\sqrt{\\sum_i (X_i - Y_i)^2}$\n",
    "- cv2.NORM_L2SQR: $\\| X-Y \\| _{L_{2S}} = \\sum_i (X_i - Y_i)^2$\n",
    "- cv2.NORM_HAMMING: Calculates the Hamming distance (count the non-zero bits in $X_i \\mathbin{\\&} Y_i$, where $\\mathbin{\\&}$ is a bit-wise \"and\") between the arrays.\n",
    "- cv2.NORM_HAMMING2: Similar to `NORM_HAMMING`, but in the calculation, each two bits of the input sequence will be added and treated as a single bit to be used in the same calculation as `NORM_HAMMING` (only useful if you set the `WTA_K` parameter of ORB to something else than `2`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brute force (BF) matcher\n",
    "It has only 1 parameter, beside the distance function: `crossCheck`. It allows to perform a symmetry test, i.e. to keep only descriptors pairs where each one is the closest to the other one in each set, or more formally:\n",
    "$$\n",
    "\\{\n",
    "(\\hat{d_i},\\hat{d_j}) \\mid\n",
    "\\hat{d_j} = \\underset{d_j \\in D_2}{\\mathrm{argmin}} \\operatorname{dist}(\\hat{d_i}, d_j)\n",
    "\\land\n",
    "\\hat{d_i} = \\underset{d_i \\in D_1}{\\mathrm{argmin}} \\operatorname{dist}(d_i, \\hat{d_j})\n",
    "\\},\n",
    "$$\n",
    "otherwise, we get the following set, $\\forall d_i \\in D_1$:\n",
    "$$\n",
    "\\{\n",
    "(d_i,\\hat{d_j}) \\mid\n",
    "\\hat{d_j} = \\underset{d_j \\in D_2}{\\mathrm{argmax}} \\operatorname{score}(d_i, d_j)\n",
    "\\}.\n",
    "$$\n",
    "\n",
    "We recommend to create a BF matcher using `cv2.BFMatcher_create(normType, crossCheck)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FLANN-based matcher\n",
    "FLANN stands for *Fast Library for Approximate Nearest Neighbors*.\n",
    "\n",
    "The FLANN-based matcher is much more complex than the BF one, as it can use multiple indexing strategies \n",
    "(which may or may not be compatible with your descriptor type!) which have, in turn, parameters to be set.\n",
    "\n",
    "This matcher may be faster when matching a large train collection than the brute force matcher.\n",
    "\n",
    "A good but old documentation is available for \n",
    "[OpenCV 2.4 implementation](https://docs.opencv.org/2.4/modules/flann/doc/flann_fast_approximate_nearest_neighbor_search.html).\n",
    "\n",
    "OpenCV supports several indexing algorithms:\n",
    "- Linear: the index will perform a linear, brute-force search.\n",
    "  - algorithm code: FLANN_INDEX_LINEAR = 0\n",
    "  - no extra parameters\n",
    "- KD-Trees: the index constructed will consist of a set of randomized kd-trees which will be searched in parallel.\n",
    "  - algorithm code: FLANN_INDEX_KDTREE = 1\n",
    "  - extra parameters: `trees` The number of parallel kd-trees to use. Good values are in the range `[1..16]`.\n",
    "- K-Means: the index constructed will be a hierarchical k-means tree.\n",
    "  - algorithm code: FLANN_INDEX_KMEANS = 2\n",
    "  - extra parameters include the branching factor to use for the hierarchical k-means tree, \n",
    "  and parameters for K-Means initialization and computation.\n",
    "- Composite: the index created combines the randomized kd-trees and the hierarchical k-means tree.\n",
    "  - algorithm code: FLANN_INDEX_COMPOSITE = 3\n",
    "  - extra parameters include both previous parameters sets.\n",
    "\n",
    "- KD-Tree: the index is contructed using a single KD-tree.\n",
    "  - algorithm code: FLANN_INDEX_KDTREE_SINGLE = 4\n",
    "  - extra parameters: None, apparently\n",
    "- Hierarchical clustering: Documentation is missing, but it seems to be a classical hierarchical clustering.\n",
    "  - algorithm code: FLANN_INDEX_HIERARCHICAL = 5\n",
    "  - extra parameters: unclear\n",
    "- LSH (Locality Sensitive Hashing): the index created uses multi-probe LSH.\n",
    "  - algorithm code: FLANN_INDEX_LSH = 6\n",
    "  - **This indexing algorithm is compatible with ORB's binary descriptors!**\n",
    "  - extra parameters:\n",
    "    - `table_number`: the number of hash tables to use (between 10 and 30 usually).\n",
    "    - `key_size`: the size of the hash key in bits (between 10 and 20 usually).\n",
    "    - `multi_probe_level`: the number of bits to shift to check for neighboring buckets (0 is regular LSH, 2 is recommended).\n",
    "  - Also, [this tutorial](https://docs.opencv.org/master/dc/dc3/tutorial_py_matcher.html) suggests to use the following parameters:\n",
    "      - `table_number = 6`\n",
    "      - `key_size = 12`\n",
    "      - `multi_probe_level = 1`\n",
    "- and [even more](https://github.com/opencv/opencv/blob/b39cd06249213220e802bb64260727711d9fc98c/modules/flann/include/opencv2/flann/defines.h#L68)…\n",
    "\n",
    "This matcher also has search parameters (like whether to sort the results) but there are very little reasons to change the default values.\n",
    "\n",
    "To create a FLANN-based matcher, we recommend to use the following technique:\n",
    "```python\n",
    "# Create a dictionary for indexing parameters:\n",
    "flann_index_params= dict(algorithm = 6, # LSH\n",
    "                        table_number = 6, # LSH parameters\n",
    "                        key_size = 12,\n",
    "                        multi_probe_level = 1)\n",
    "# Then create the matcher\n",
    "matcher = cv2.FlannBasedMatcher(indexParams=flann_index_params)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Create a BF matcher and FLANN matcher.**\n",
    "</div>\n",
    "\n",
    "*Hint:* keep in mind that your ORB descriptors will be **binary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# matcher_BF = ...\n",
    "# matcher_FLANN = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexation\n",
    "While it is possible to directly call  `matcher.match(descriptors1, descriptors2)`,\n",
    "we usually index descriptors before matching them.\n",
    "\n",
    "This is useful in real conditions for the case we are working on: we have to match each frame against every possible model (there were severa songs available), so this allows to:\n",
    "1. perform indexing only once;\n",
    "2. handle multiple models and therefore perform object detection (however the pipeline is a bit more complex).\n",
    "\n",
    "This is performed using the `matcher.add(list_of_list_of_descriptors)` \n",
    "which adds sets of descriptors for several **training (or \"model\") images**.\n",
    "\n",
    "The index then retains for each single descriptor:\n",
    "- its value (indexed);\n",
    "- the id of the training image.\n",
    "\n",
    "We will therefore distinguish between:\n",
    "- **train descriptors**, provided upon training;\n",
    "- **query descriptors**, provided upon matching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Index the descriptors of the model image.**\n",
    "\n",
    "*Tips:*\n",
    "- Use `matcher.add()`.\n",
    "- Do it for both matchers (we will compare them).\n",
    "- **add() takes a list of list of descriptors!**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# matcher_BF.add(...)\n",
    "# matcher_FLANN.add(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Match descriptors and estimate the homography\n",
    "We are now ready to match descriptors.\n",
    "\n",
    "We suggest to use a FLANN-based matcher to be able to perform a ratio-test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching descriptors\n",
    "Matching descriptors is performed using one of the following functions:\n",
    "\n",
    "- `matcher.match(query_descriptors)`: actually is `knnMatch`, with `k=1`.\n",
    "- `matcher.knnMatch(query_descriptors, k=...)`: \n",
    "  Performs a K-nearest neighbor search for a given query point using the index.\n",
    "  - Without symmetry test, this will return a match for each query descriptor as long as there is at least one descriptor in the train set.\n",
    "  - Useful for ratio test with `k` > 1.\n",
    "  - Note that `k` > 1 is not possible with BF matcher when `crossCheck` is `True`!\n",
    "- `matcher.radiusMatch(query_descriptors, maxDistance=...)`: \n",
    "  Performs a radius nearest neighbor search for a given query point, ie returning only results within the specified radius).\n",
    "  - Useful with when we have a background model which allows us to set a threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Simple match"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the matches between the frame descriptors and the model descriptors using the FLANN matcher.**\n",
    "\n",
    "*Tips:*\n",
    "- Use the `matcher.match()` method to avoid having a list of tuples of 1 element as result.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute the matches\n",
    "# matches = ...\n",
    "# len(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the matches\n",
    "Here is a simple way to display the matches using `cv2.drawMatches()`.\n",
    "We could keep only the closest matches, but we will keep this simple for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me\n",
    "def draw_matches(img1, kpts1, img2, kpts2, matches, color=(0,0,255), title=\"\"):\n",
    "    '''img1 and img2 are color images.'''\n",
    "    img_matches = np.empty((max(img1.shape[0], img2.shape[0]),\n",
    "                           img1.shape[1]+img2.shape[1], \n",
    "                           3), \n",
    "                           dtype=np.uint8)\n",
    "    img_matches = cv2.drawMatches(img1, kpts1, img2, kpts2, \n",
    "                          matches, \n",
    "                          img_matches,\n",
    "                          matchColor=color,\n",
    "                          flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.imshow(bgr2rgb(img_matches))\n",
    "    plt.title(title + \" - %d matches\" % (len(matches),))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Draw those first matches (frame → model) in RED.**\n",
    "</div>\n",
    "\n",
    "\n",
    "Expected result of `draw_matches()`:\n",
    "![](img/practice_03/out3_matches_frame-to-model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO draw the first matches\n",
    "# draw_matches(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Symmetry test\n",
    "Let us now use the BF matcher to ask for a cross check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the matches using a symmetry test and display them in BLUE.**\n",
    "</div>\n",
    "\n",
    "\n",
    "Expected result of `draw_matches()`:\n",
    "![](img/practice_03/out4_matches_frame-to-model-sym.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# matches = ...\n",
    "# draw_matches(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Ratio test\n",
    "Let's stop using the BF matcher now, and **use the FLANN matcher for what remains.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute the matches using the FLANN-based matcher, asking for the 2 nearest neighbors.**\n",
    "</div>\n",
    "\n",
    "*Hint:* `matches` will contain a list of pairs of matches, as opposed to single matches in previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# matches = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Match results\n",
    "\n",
    "The result of `matches = matcher.match*(query_descriptors)` line is a list of `DMatch` objects. \n",
    "A `DMatch` object has following attributes:\n",
    "- `DMatch.distance`: Distance between descriptors. The lower, the better it is.\n",
    "- `DMatch.trainIdx`: Index of the descriptor in train descriptors\n",
    "- `DMatch.queryIdx`: Index of the descriptor in query descriptors\n",
    "- `DMatch.imgIdx`: Index of the train image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Filter the matches using a ratio test.**\n",
    "\n",
    "*Tips:*\n",
    "- This means, $\\forall \\texttt{m},\\texttt{n} \\in M$ keep $\\texttt{m}$ only if $\\texttt{m.distance} < \\texttt{n.distance} * T$ where $T$ is the ratio test value.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO filter matches\n",
    "# good_matches = ...\n",
    "# len(good_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Draw those good matches (frame → model) with ratio test in CYAN.**\n",
    "</div>\n",
    "\n",
    "\n",
    "Expected result of `draw_matches()`:\n",
    "![](img/practice_03/out5_matches_frame-to-model-ratio.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# draw_matches(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compare the filtering of the symmetry test and the ratio test: which one rejects more matches?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Geometric validation\n",
    "Finally, using the good matches we computed using the ratio test, we can estimate the perspective transform between the model and the frame (in this direction, because we will project a modified model image over the scene/frame).\n",
    "\n",
    "First we need to build two corresponding lists of point coordinates, for the source and for the destination."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Extract the point coordinates of the good matches to build two list of corresponding coordinates in the model and in the frame referentials.**\n",
    "\n",
    "*Tips:*\n",
    "- Recover the index of the keypoints using either `m.trainIdx` or `m.queryIdx`.\n",
    "- Extract the point coordinates from each keypoint using `kpts[INDEX].pt`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prof\n",
    "pts_mdl = []\n",
    "pts_frame = []\n",
    "# for m in good_matches:\n",
    "#     # TODO\n",
    "len(pts_mdl), len(pts_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the RANSAC implementation in OpenCV requires float numbers, we will convert our coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run me\n",
    "pts_mdl, pts_frame = np.float32(pts_mdl), np.float32(pts_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to estimate the homography using RANSAC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Use `cv2.findHomography()` to estimate the homography from the model to the frame.**\n",
    "\n",
    "*Tips:*\n",
    "- The constant to use the RANSAC method is `cv2.RANSAC`.\n",
    "- `3` is a good value for the RANSAC retroprojection error threshold which rejects point pairs if\n",
    "$$\n",
    "\\| \\texttt{dstPoints} _i -  \\texttt{convertPointsHomogeneous} ( \\texttt{H} * \\texttt{srcPoints} _i) \\|_2\n",
    ">  \\texttt{ransacReprojThreshold}.\n",
    "$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.findHomography?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# H, pts_inliers_mask = cv2.findHomography(...)\n",
    "# H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Filter the good matches to keep only the RANSAC inliers.**\n",
    "\n",
    "*Tips:*\n",
    "- `pts_inliers_mask` indicates which point pairs are inliers.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO \n",
    "# matches_ransac_inliers = ...\n",
    "# len(matches_ransac_inliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Draw those good inlier matches (frame → model) with ratio test and RANSAC in GREEN.**\n",
    "</div>\n",
    "\n",
    "\n",
    "Expected result of `draw_matches()`:\n",
    "![](img/practice_03/out6_matches_frame-to-model-ransac.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# draw_matches(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Simple AR\n",
    "Finally, we can project some image over the frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model quadrilateral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Define an array of shape `(1, 4, 2)` and type `np.float32` to represent the coordinates of the 4 corners of the model.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# model_quad = np.float32([[[0, 0],\n",
    "#                          ...]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame quadrilateral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Now use `cv2.perspectiveTransform()` to compute the coordinates of the model corners within the frame referential.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# frame_quad = cv2.perspectiveTransform(...)\n",
    "# frame_quad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the object outline\n",
    "\n",
    "We can now draw the detected object over the frame.\n",
    "\n",
    "Expected result:\n",
    "![](img/practice_03/out7_frame_outline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbg_img = frame_img.copy()\n",
    "cv2.polylines(dbg_img, np.int32(frame_quad), True, (0, 255, 0), 10)\n",
    "plt.imshow(bgr2rgb(dbg_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project a modifier model image on the scene (the frame)\n",
    "Let us use a very simple modified model image, to indicate we detected it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_img_modified = np.uint8(model_img * (1,1,0))\n",
    "plt.imshow(bgr2rgb(model_img_modified))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Use `cv2.warpPerspective()` to project `model_img_modified` onto the frame's referential.**\n",
    "\n",
    "*Tips:*\n",
    "- **Warning:** the `dsize` takes a `tuple(int, int)` in the form `(num_columns, num_rows)`, and **not** `(rows, cols)` as in the shape of a row-major NumPy array!\n",
    "\n",
    "</div>\n",
    "\n",
    "Expected output:\n",
    "![](img/practice_03/out_warpperspective.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.warpPerspective?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# warped_img = cv2.warpPerspective(...)\n",
    "# plt.imshow(bgr2rgb(warped_img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to use a mask to blend this warped image with the original frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Create a mask with `np.zeros` and fill the right region using `cv2.fillPoly()`.**\n",
    "</div>\n",
    "\n",
    "Expected output:\n",
    "![](img/practice_03/out_fillpoly.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.fillPoly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# warped_img_msk= np.zeros(...)\n",
    "# warped_img_msk = cv2.fillPoly(...)\n",
    "# plt.imshow(warped_img_msk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Finally, overlay the modified image over the frame.**\n",
    "</div>\n",
    "\n",
    "Expected output:\n",
    "![](img/practice_03/out_final_ar.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BONUS Mobile document scanner\n",
    "Assume you have the four coordinates of the corners of the documents in the frame (they are in `frame_quad.squeeze()`), and that it is a landscape A4 page (you have its corners in `model_quad.squeeze()`), create a dewarped (cropped, without perspective) document image.\n",
    "\n",
    "Said differently: Knowing the model shape, from the coordinates of the object\n",
    "![Input](img/practice_03/bonus-input.png)\n",
    "\n",
    "produce the following cropped image:\n",
    "![output](img/practice_03/bonus-result.png)\n",
    "\n",
    "\n",
    "Hints:\n",
    "- use `cv2.getPerspectiveTransform`\n",
    "\n",
    "Extra kudos:\n",
    "- adjust the size of the output image depending on the area on the frame."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
