{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EPITA 2023 MLRF practice_04-01_BoVW v2023-06-06_192651 by Joseph CHAZALON\n",
    "\n",
    "<div style=\"overflow: auto; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"Creative Commons License\" src='img/CC-BY-4.0.png' style='float: left; margin-right: 20px'>\n",
    "    \n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice 04 part 01: Bag of Visual Words search engine\n",
    "\n",
    "We will demonstrate how to use this classical technique to build a global descriptor from local descriptors.\n",
    "\n",
    "In this session, you will learn how to produce results like the ones displayed below (left column: queries, other columns: responses to query in the same row).\n",
    "\n",
    "![Sample output](img/practice_04/sample_results.jpg)\n",
    "\n",
    "\n",
    "We will proceed in 9 steps:\n",
    "\n",
    "1. Sample some descriptors for codebook learning\n",
    "2. Learn normalisation parameters for descriptors (mean and eigenvectors)\n",
    "3. Use k-Means to learn a codebook\n",
    "4. Compute the BoVW vector for each image\n",
    "5. Setup a nearest neighbors search structure\n",
    "6. Evaluate our approach using mean average precision\n",
    "7. Display some results\n",
    "8. Compute the best results for the test queries\n",
    "9. Export the results for the test queries (and submit them for grading).\n",
    "\n",
    "\n",
    "## Resources\n",
    "The resources for this session are distributed as a separate archive.\n",
    "Here is a summary of the files we provide:\n",
    "- `jpg/*.jpg`: 1491 images;\n",
    "- `thumbs/*.jpg`: image thumbnails for fast display;\n",
    "- `siftgeo/*.siftgeo`: associated pre-computed SIFT descriptors for each image;\n",
    "- `gt_student.json`: a partial ground truth to enable a partial evaluation of the retrieval;\n",
    "- `queries_for_grading.json`: a list of queries for which you need to submit your results.\n",
    "\n",
    "\n",
    "## Ground truth format\n",
    "The ground truth file is a JSON file. Here is an except:\n",
    "```json\n",
    "{\n",
    " \"90314\": [\n",
    "  \"90989\",\n",
    "  \"91259\"\n",
    " ],\n",
    " \"90376\": [\n",
    "  \"90674\"\n",
    " ],\n",
    " ...\n",
    "}\n",
    "```\n",
    "It contains a dictionary which associates to each query the list of relevant elements.\n",
    "Every value is a string representing an image identifier, ie the part of the image or descriptor file without extension.\n",
    "\n",
    "Example: `\"90314\"` is the identifier\n",
    "of the image `jpgs/90314.jpg`\n",
    "with descriptors available at `siftgeo/90314.siftgeo`\n",
    "and its thumbnail is `thumbs/90314.jpg`.\n",
    "\n",
    "Looking at the previous excerpt, we can see that the query `90314` has two relevant results: `90989` and `91259`. \n",
    "\n",
    "\n",
    "## Local descriptors\n",
    "We provide precomputed SIFT descriptors to save you time.\n",
    "We also provide a commodity function to help you load them (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import module, load resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXME setup the resources location\n",
    "PATH_TO_RESOURCES = \"/afs/cri.epita.fr/resources/teach/bigdata/mlrf21/INRIA_Holidays_shuffled/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to read descriptors\n",
    "We provide you with the `siftgeo_read_desc(path)` function which reads the list of descriptors stored in a `.siftgeo` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIFTGEO_DTYPE = np.dtype([\n",
    "    (\"x\", \"<f4\"),\n",
    "    (\"y\", \"<f4\"),\n",
    "    (\"scale\", \"<f4\"),\n",
    "    (\"angle\", \"<f4\"),\n",
    "    (\"mi11\", \"<f4\"),\n",
    "    (\"mi12\", \"<f4\"),\n",
    "    (\"mi21\", \"<f4\"),\n",
    "    (\"mi22\", \"<f4\"),\n",
    "    (\"cornerness\", \"<f4\"),\n",
    "    (\"desdim\", \"<i4\"),\n",
    "    (\"component\", \"<u1\", 128)\n",
    "])\n",
    "\n",
    "def siftgeo_read_full(path):\n",
    "    return np.fromfile(path, dtype=SIFTGEO_DTYPE)\n",
    "\n",
    "def siftgeo_read_desc(path):\n",
    "    desc = siftgeo_read_full(path)[\"component\"]\n",
    "    if desc.size == 0: \n",
    "        desc = np.zeros((0, 128), dtype = 'uint8')\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some of utility listings\n",
    "Again, to save you time, here are some useful lists and mappings:\n",
    "- `IMG_NAMES_FULL`: list of all full images paths\n",
    "- `IMG_IDS`: list of image ids (filename without directories nor extension)\n",
    "- `imgid_to_index`: a mapping from image ids (like `'90000'`, `'90001'`, `'90002'` or `'90003'` — note the quotes indicating string types) to an absolute index in the image list (like `0`, `1`, `2` or `3` — integers). **This will be useful to convert between image paths and rows in the global index.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_NAMES_FULL = !ls $PATH_TO_RESOURCES/thumbs/*.jpg | sort\n",
    "len(IMG_NAMES_FULL), IMG_NAMES_FULL[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_IDS = [p.split('/')[-1][:-4] for p in IMG_NAMES_FULL]\n",
    "len(IMG_IDS), IMG_IDS[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imgid_to_index: str             --> int\n",
    "#                 image base name |-> rank of the content associated to this image in our various lists\n",
    "imgid_to_index = {imgid: ii for ii, imgid in enumerate(IMG_IDS)}\n",
    "imgid_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Sample some descriptors for codebook learning\n",
    "Because RAM is not that cheap, loading all descriptors in RAM and fitting a k-Means on them would require very complex implementation not available on our regular libraries.\n",
    "\n",
    "We will select some images and load the descriptors for those images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Randomly select 100 siftgeo files and load the descriptors from them. We will call those descriptors `train_desc`.**\n",
    "    \n",
    "*Hint: you can reuse what you did to select random RGB triplets (pixels) in `practice_02-01_color-histogram.ipynb`.*\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO sample some images\n",
    "# sample_images = np.random.choice(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO load (all) the descriptors for those images\n",
    "# train_desc = []\n",
    "# for imid in sample_images:\n",
    "#     ...\n",
    "#    load the descriptors\n",
    "#    append them in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us look at the list of numpy arrays we just created\n",
    "print(f\"We loaded the descriptors of {len(train_desc)} images.\")\n",
    "for ii in range(3):\n",
    "    print(f\"For image {ii}, we read {train_desc[ii].shape[0]} descriptors.\"\n",
    "          f\" Each of them has {train_desc[ii].shape[1]} components stored as {train_desc[ii].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should print something like:\n",
    "```\n",
    "We loaded the descriptors of 100 images.\n",
    "For image 0, we read 514 descriptors. Each of them has 128 components stored as uint8\n",
    "For image 1, we read 5146 descriptors. Each of them has 128 components stored as uint8\n",
    "For image 2, we read 1208 descriptors. Each of them has 128 components stored as uint8\n",
    "```\n",
    "\n",
    "You should take some time to anwser the following questions:\n",
    "\n",
    "1. Do you have the same number of descriptors than in the previous text for each image?\n",
    "2. Is there the same number of descriptors for all images?\n",
    "3. What is the size of a single SIFT descriptor (in bytes)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Prepare those descriptors for training\n",
    "For those descriptors to be helpful, we need them to be staked in a big single array of shape `(num_desc, desc_len)` where `desc_len` is the length of a single descriptor — here 128 because SIFT decriptors are vectors of 128 integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Turn your list of arrays of descriptors in a big single array. Its shape should be something like `(314302, 128)`.**\n",
    "\n",
    "Hint: look as `numpy.*stack` functions.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO stack all you descriptors into one big array\n",
    "# train_desc = ... # FIXME\n",
    "# train_desc.shape, train_desc.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we convert this array to float32 elements to avoid normalization issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_desc = train_desc.astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learn normalisation parameters for descriptors\n",
    "Before learning a codebook, it is a good practice to:\n",
    "- center the features;\n",
    "- reduce their dimension using PCA.\n",
    "\n",
    "**Warning**: We do NOT REDUCE (divide by the variance) the descriptors here because we want to keep the relative value of the variables. This is indeed the very purpose of histograms! **If you have time, you can provide a comparison with/without reduction in your report for extra point(s).**\n",
    "\n",
    "**This is a first learning step!** Because we will compute a **mean** and **eigenvectors** on a **training set**, we need to **store those values** in order to be able to apply them to test elements later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Centering descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Compute and store the mean of your descriptors, then center them.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO compute mean and center descriptors\n",
    "# train_mean = ...\n",
    "# train_desc = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 PCA\n",
    "We will now compute the PCA parameters which will allow us to reduce the dimension of our descriptors from 128 floats to 64 floats, keeping as much information as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute PCA matrix and keep only 64 dimensions\n",
    "train_cov = np.dot(train_desc.T, train_desc)\n",
    "eigvals, eigvecs = np.linalg.eig(train_cov)\n",
    "perm = eigvals.argsort()                   # sort by increasing eigenvalue\n",
    "pca_transform = eigvecs[:, perm[64:128]]   # eigenvectors for the 64 last eigenvalues\n",
    "pca_transform.shape, pca_transform.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Apply the PCA transformation to your descriptors.**\n",
    "\n",
    "Hint: we expect to get a new `train_desc` array with the same number of rows, but only 64 columns (dimensions). You can use sklearn's preprocessing tools if you prefer.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# perform the appropriate matrix multiplication to transform your descriptors\n",
    "# train_desc = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the shape of your new descriptors: we expect a shape like (BIGNUMBER, 64)\n",
    "train_desc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use k-Means to learn a codebook\n",
    "We are now ready to learn a codebook using those centered and compact descriptors.\n",
    "We will use exactly the same technique as the one we used to compute color histograms:\n",
    "- first we fit a k-Means;\n",
    "- then, using the centroids we found, we will project the descriptors and compute bag of features for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Fit a k-Means with 512 clusters on the training descriptors.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO fit a k-means (this should be fast; e.g. 5 secondes on a laptop with a gen 8 i7 CPU and 16 GB of RAM)\n",
    "# kmeans = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the shape of the cluster's centers\n",
    "kmeans.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect a shape of `(512, 64)` for cluster centers: we have 512 clusters in a 64-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**How do we call this set of cluster centers? They define the set of possible descriptor values we will use to build bag of visual words descriptors. Write down your answer.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO \n",
    "We call this set of cluster centers ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute the BoVW vector for each image\n",
    "Using the codebook we just learned, we will compute for each image its bag of features vector (histogram of projected descriptors).\n",
    "\n",
    "It is almost exaclty like the color histogram we computed in practice session 2!\n",
    "The **only** difference is that we have much more dimensions.\n",
    "\n",
    "We will put those descriptors in a naive index made of a single NumPy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**For each image, compute its BoVW vector.**\n",
    "</div>\n",
    "\n",
    "**Beware:**\n",
    "- in a few images no descriptor was extracted so you need to skip those elements\n",
    "- you need to apply the same preprocessings than the ones you applied to training descriptors (center, dimension reduction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO index all images\n",
    "\n",
    "# Here is our very simple index (keep it initialized like this)\n",
    "global_image_descr = np.zeros((len(IMG_IDS), kmeans.n_clusters), dtype=np.float32)\n",
    "\n",
    "for ii, imgid in enumerate(IMG_IDS):\n",
    "    print(\"Indexing %s\" % (imgid,))\n",
    "    # read the descriptors\n",
    "    # desc = ...  # FIXME\n",
    "    \n",
    "    # handle case where no descriptor is available\n",
    "    if desc.shape[0] == 0:\n",
    "        # let the descriptor be 0 for all values\n",
    "        # note that this is bad and the element should be dropped from the index\n",
    "        # but here we keep it to preserve image vs desc ids (no worries, it will work)\n",
    "        print(\"WARNING: zero descriptor for %s\" % (imgid,))\n",
    "        continue\n",
    "    \n",
    "    # apply the same preprocessing as for training descriptors\n",
    "    # ...   # FIXME\n",
    "\n",
    "    # get cluster ids\n",
    "    # ...  # FIXME\n",
    "    \n",
    "    # compute histogram (do not forget to normalize by the number of descriptors)\n",
    "    # descr_hist = ...  # FIXME\n",
    "    \n",
    "    # update the index\n",
    "    # global_image_descr[ii] = descr_hist   # FIXME (just uncomment)\n",
    "print(\"Indexing complete.\")\n",
    "global_image_descr.shape, global_image_descr.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transform.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup a nearest neighbors search structure\n",
    "We will use a simple nearest neighbors implementation to test our approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Using scikit-learn's implementation of nearest neighbors, setup a search engine with a linear search strategy and an appropriate metric.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO setup search engine\n",
    "# search_engine = NearestNeighbors(algorithm=???, metric=???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Now index you descriptors.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO index descriptors\n",
    "# search_engine. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate our approach using mean average precision\n",
    "We can now evaluate the performance of our search engine using the available ground truth.\n",
    "\n",
    "### 6.1 Load the ground truth\n",
    "The ground truth is a simple dictionary mapping query ids to relevant image ids:\n",
    "```json\n",
    "{'90314': ['90989', '91259'],\n",
    " '90376': ['90674'],\n",
    " '90175': ['90512'],\n",
    " ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_GT = os.path.join(PATH_TO_RESOURCES, \"gt_student.json\")\n",
    "gt_data = None\n",
    "with open(PATH_TO_GT, 'r') as in_gt:\n",
    "    gt_data = json.load(in_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Select the descriptors of the query images\n",
    "\n",
    "The annoying part is to convert between the indices of the elements in our index structure `global_image_descr` and the images ids which allow us to find files.\n",
    "\n",
    "The upside is that reusing the precomputed descriptors avoids us the need for computing the BoVW vector for each query image.\n",
    "\n",
    "We use the `imgid_to_index`, which helps us converting between image ids (filename without directory nor extension) and index value in the `global_image_descr` array, to recover the indices of the rows corresponding to the query images in `global_image_descr`.\n",
    "\n",
    "Here is below some code you should run and try to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices of the query images\n",
    "query_imnos = [imgid_to_index[query_id] for query_id in gt_data.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgid_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_imnos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_IDS[query_imnos[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_data[IMG_IDS[query_imnos[0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Using `query_imnos`, you can now select the descriptors of the query images from the precomputed array `global_image_descr`.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO select the descriptors of the query images\n",
    "# query_vectors = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Run the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Run a search query for all query images using our search engine. Ask for 9 neighbors because we want 8 real results and we will later remove the query from the list of results (we expect it the be the first result here, because our queries belong to our database for testing purposes).**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# get the 9 (8 real) NNs for all query images\n",
    "# distances, results = search_engine...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect here a shape of `(250, 9)` for the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Evaluate the performance using mAP\n",
    "The mean average precision is the mean of the AP computed for each query.\n",
    "The AP is the area under the precision-recall curve.\n",
    "\n",
    "For each query, we will therefore sum the areas of the trapezoids between each recall values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Complete the code below to compute the mean average precision of our search engine on the training set.**\n",
    "\n",
    "Hint: you should get a value much higher than what random results would produce ($0.001$ in our tests but you can simulate this yourself).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO complete this code\n",
    "aps = []  # list of average precisions for all queries\n",
    "for qimno, qres in zip(query_imnos, results):\n",
    "    qname = IMG_IDS[qimno]\n",
    "#     print(\"query:\", qname)\n",
    "    # collect the positive results in the dataset\n",
    "    # the positives have the same prefix as the query image\n",
    "    positive_results = [imgid_to_index[img_id] for img_id in gt_data[IMG_IDS[qimno]]]\n",
    "#     print(\"positive_results:\", positive_results)\n",
    "#     print(\"qres:\", qres)\n",
    "    #\n",
    "    # ranks of positives. We skip the result #0, assumed to be the query image\n",
    "    ranks = [i for i, res in enumerate(qres[1:]) if res in positive_results]\n",
    "#     print(\"ranks:\", ranks)\n",
    "    #\n",
    "    # accumulate trapezoids with this basis\n",
    "    recall_step = 1.0 / len(???)  # FIXME what is the size of a step?\n",
    "    ap = 0\n",
    "    for ntp, rank in enumerate(ranks):\n",
    "        # ntp = nb of true positives so far\n",
    "        # rank = nb of retrieved items so far\n",
    "        # y-size on left side of trapezoid:\n",
    "        precision_0 = ntp/float(rank) if rank > 0 else 1.0\n",
    "        # y-size on right side of trapezoid:\n",
    "        precision_1 = (ntp + 1) / float(rank + 1)\n",
    "        ap += ???  # FIXME what is the area under the PR curve?\n",
    "    print(\"query %s, AP = %.3f\" % (qname, ap))\n",
    "    aps.append(ap)\n",
    "\n",
    "print(\"mean AP = %.3f\" % ???)  # FIXME mean average precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Display some results\n",
    "We provide you with some code to display query results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "\n",
    "nrow = 8   # number of query images to show\n",
    "nres = 8   # number of results per query\n",
    "\n",
    "def show_image(imno, frame_color):\n",
    "    im = imread(os.path.join(PATH_TO_RESOURCES, \"thumbs\", \"%s.jpg\" % IMG_IDS[imno]))\n",
    "    plt.imshow(im)\n",
    "    h, w = im.shape[:2]\n",
    "    plt.plot([0, 0, w, w, 0], [0, h, h, 0, 0], frame_color, linewidth = 2)\n",
    "    plt.axis('off')\n",
    "\n",
    "# reduce the margins\n",
    "plt.subplots_adjust(wspace = 0, hspace = 0,\n",
    "                    top = 0.99, bottom = 0.01, left = 0.01, right = 0.99)\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "no = 1  # index current of subfigure\n",
    "for qno in range(nrow):\n",
    "    plt.subplot(nrow, nres + 1, no); no += 1\n",
    "    # show query image with white outline\n",
    "    qimno = query_imnos[qno]\n",
    "    show_image(qimno, 'w')\n",
    "    plt.title(IMG_IDS[qimno])\n",
    "    for res_rank, qres in enumerate(results[qno, 1:nres+1]): # 1:nres+1 => skip query from results\n",
    "        plt.subplot(nrow, nres + 1, no); no += 1\n",
    "        # use image name to determine if it is a TP or FP result\n",
    "        is_ok = IMG_IDS[qres] in gt_data[IMG_IDS[qimno]]\n",
    "        show_image(qres, 'g' if is_ok else 'r')\n",
    "#         plt.title(\"%0.2f\" % (distances[qno,res_rank]))\n",
    "        plt.title(IMG_IDS[qres])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Compute the best results for the test queries\n",
    "You are now ready to process the test data for the grading.\n",
    "\n",
    "We provide you with a set of query ids, and you must produce the list of most releveant results for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = None\n",
    "with open(os.path.join(PATH_TO_RESOURCES, \"queries_for_grading.json\")) as qin:\n",
    "    test_queries = json.load(qin)\n",
    "test_queries[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**For each query, compute the list of best results.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO everything\n",
    "# ...\n",
    "# distances, test_results = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"work\" src='img/work.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**Display your results to control them visually. Make sure you really perform this step because the train and test sets have the same size here, and it is easy to get confused and submit the wrong set!**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"overflow: auto; border-style: dotted; border-width: 1px; padding: 10px; margin: 10px 0px\">\n",
    "<img alt=\"warning\" src='img/warning.png' style='float: left; margin-right: 20px'>\n",
    "\n",
    "**The train and test set have the same size! Make sure you submit results for the test set and not for the train set.**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Export the results for the test queries\n",
    "\n",
    "Here is some code to export your results in the appropriate format.\n",
    "\n",
    "**Do not forget to submit them!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {query_id: [IMG_IDS[ri] for ri in results_ids[1:]]\n",
    "           for query_id,results_ids in zip(test_queries, test_results)}\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPORT_PATH = \"results.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(EXPORT_PATH, 'w') as outres:\n",
    "    json.dump(results, outres, indent=1)\n",
    "!head $EXPORT_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job done!\n",
    "Do you think you can improve the performance of your approach and get a better grading?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 331,
   "position": {
    "height": "353px",
    "left": "838px",
    "right": "20px",
    "top": "207px",
    "width": "542px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
